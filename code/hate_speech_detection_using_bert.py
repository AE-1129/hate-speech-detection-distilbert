# -*- coding: utf-8 -*-
"""Hate Speech Detection using BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-AoW593yPp_80vsXvhjGOmskieoT7-1W
"""

!pip install transformers datasets scikit-learn matplotlib pandas tqdm

import json, requests, random
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification
from torch.optim import AdamW
from tqdm import tqdm
import matplotlib.pyplot as plt

# -------------------------------
# 1. Load HateXplain dataset
# -------------------------------
url = "https://raw.githubusercontent.com/hate-alert/HateXplain/master/Data/dataset.json"
data_json = requests.get(url).json()

records = []
for post in data_json.values():
    labels = [a["label"] for a in post["annotators"] if "label" in a]
    if not labels:
        continue
    maj = max(set(labels), key=labels.count)
    text = " ".join(post["post_tokens"])
    gender = post.get("demographic", {}).get("gender", "neutral")
    records.append({"text": text, "label": maj, "gender": gender})

df = pd.DataFrame(records)
print("Raw dataset shape:", df.shape)
print(df['label'].value_counts())

# -------------------------------
# 2. Balance dataset
# -------------------------------
min_size = df['label'].value_counts().min()
df_balanced = df.groupby('label').apply(lambda x: x.sample(min_size, random_state=42)).reset_index(drop=True)
print("\nBalanced dataset:", df_balanced['label'].value_counts())

# -------------------------------
# 3. Label encoding
# -------------------------------
label2id = {"hatespeech": 0, "offensive": 1, "normal": 2}
df_balanced["label_id"] = df_balanced["label"].map(label2id)

# -------------------------------
# 4. Train / Test split
# -------------------------------
train_texts, test_texts, train_labels, test_labels, train_genders, test_genders = train_test_split(
    df_balanced["text"].tolist(),
    df_balanced["label_id"].tolist(),
    df_balanced["gender"].tolist(),
    test_size=0.2,
    random_state=42,
    stratify=df_balanced["label_id"]
)

# -------------------------------
# 5. Dataset & Tokenizer
# -------------------------------
tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

class HateDataset(Dataset):
    def __init__(self, texts, labels):
        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = HateDataset(train_texts, train_labels)
test_dataset = HateDataset(test_texts, test_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# -------------------------------
# 6. Model setup
# -------------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=3)
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

# -------------------------------
# 7. Training loop
# -------------------------------
epochs = 1
model.train()
for epoch in range(epochs):
    loop = tqdm(train_loader, desc=f"Training Epoch {epoch+1}")
    for batch in loop:
        optimizer.zero_grad()
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        loop.set_postfix(loss=loss.item())

# -------------------------------
# 8. Evaluation
# -------------------------------
model.eval()
y_true, y_pred = [], []
with torch.no_grad():
    for batch in tqdm(test_loader, desc="Evaluating"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["labels"].to(device)
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

# -------------------------------
# 9. Reports & Confusion Matrix
# -------------------------------
print("\nClassification Report:")
print(classification_report(
    y_true, y_pred,
    target_names=["Hate", "Offensive", "Normal"],
    zero_division=0
))

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(cm, display_labels=["Hate", "Offensive", "Normal"])
disp.plot(cmap="Blues", xticks_rotation=20)
plt.title("Confusion Matrix - HateXplain (DistilBERT)")
plt.show()

# -------------------------------
# 10. Fairness (Gender Bias) Check
# -------------------------------
test_df = pd.DataFrame({"text": test_texts, "true": y_true, "pred": y_pred, "gender": test_genders})
print("\nGender bias summary:")
print(test_df.groupby(["gender", "pred"]).size().unstack(fill_value=0))

# -------------------------------
# Display 10 test samples with model predictions
# -------------------------------

# Map label ids to human-readable labels
id2label = {0: "Hate", 1: "Offensive", 2: "Normal"}

# Select 10 random indices from test set
num_samples = min(10, len(test_texts))
sample_indices = np.random.choice(len(test_texts), size=num_samples, replace=False)


model.eval()
with torch.no_grad():
    for idx in sample_indices:
        text = test_texts[idx]
        true_label = id2label[test_labels[idx]]

        # Tokenize single example
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        inputs = {k: v.to(device) for k, v in inputs.items()}

        # Get prediction
        outputs = model(**inputs)
        pred_label_id = torch.argmax(outputs.logits, dim=1).item()
        pred_label = id2label[pred_label_id]

        print(f"Text: {text}")
        print(f"True Class: {true_label}")
        print(f"Predicted Class: {pred_label}\n")